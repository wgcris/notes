1. Spark的安装
(1) 解压spark-2.0.2-bin-hadoop2.7.tgz, 并设置SPARK_HOME
(2) 拷贝hadoop的core-site.xml和hdfs-site.xml到spark的conf目录下
(3) mv spark-env.sh.template spark-env.sh
    mv spark-defaults.conf.template spark-defaults.conf
    mv metrics.properties.template metrics.properties
    mv log4j.properties.template log4j.properties
(4) 配置spark-env.sh, 加入如下环境变量
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
export SPARK_LOCAL_DIRS=~/data/spark
export SPARK_MASTER_IP=localhost
export SPARK_MASTER_PORT=8070
export SPARK_MASTER_WEBUI_PORT=8090
export SPARK_WORKER_CORES=1
export SPARK_WORKER_MEMORY=1g
export SPARK_WORKER_PORT=8092
export SPARK_WORKER_INSTANCES=1
export SPARK_LOG_DIR=$SPARK_HOME/logs/spark
export SPARK_PID_DIR=$SPARK_HOME/tmp/spark
(5) 配置slaves文件,写入一行localhost 
(6) 验证
http://localhost:8090/ 访问spark的web界面
输入spark-shell可以运行spark-shell环境
(7) SparkPi实例
./run-example org.apache.spark.examples.SparkPi
./bin/spark-submit --class org.apache.spark.examples.SparkPi --master spark://localhost:7077 --executor-memory 1G --total-executor-cores 1 examples/jars/spark-examples_2.11-2.0.2.jar 1000
如果提交到yarn，不需要开启spark的master和worker，但是需要启动hdfs和yarn的进程。提交命令如下。
./bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn-cluster --executor-memory 1G --total-executor-cores 1 examples/jars/spark-examples_2.11-2.0.2.jar 1000
如果在spark-default.conf里面配置spark.mastr为yarn，则在提交的时候不用指定--master
./bin/spark-submit --class org.apache.spark.examples.SparkPi --executor-memory 1G --total-executor-cores 1 examples/jars/spark-examples_2.11-2.0.2.jar 1000

		一个MapReduce程序的分析
注: mapreduce新版api, mapred是旧版api

本文以WordCount为例进行分析。
1. wordcount任务的执行
(1) hdfs dfs -put a.txt /tmp/in						注:这里txt为653M左右
(2) hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar wordcount /tmp/in /tmp/out
2. 源码分析
2.1 wordcount程序的提交
WordCount::main
	GenericOptionsParser					//分析传入参数，这里传入的参数只有两个: /tmp/in /tmp/out	
	Job.getInstance
	GenericOptionsParser					//处理传入参数加入到conf中
	Job.getInstance							//构造一个Job对象，继承于JobContextImpl
		getInstance
			构造Job对象
				设置Job的cluster字段为null
		setJobName				
	job.setJarByClass								
	job.setMapperClass
	job.setCombinerClass
	job.setReducerClass
	job.setOutputKeyClass
	job.setOutputValueClass					//以上是一些设置，设置到了jobconf中
	FileInputFormat.addInputPath
	FileOutputFormat.setOutputPath
	waitForCompletion						//org.apache.hadoop.mapreduce.Job.waitForCompletion
		submit										//只要为DEFINE状态才提交
			setUseNewAPI						
				设置numReduces，由mapreduce.job.reduces设置，默认为1				
				假如没有配置mapred.mapper.class就设置mapred.mapper.new-api为true，标记使用新的map的api
				假如没有配置mapred.reducer.class就设置mapred.reducer.new-api为true，标记使用新的reduce的api
			connect									//构造josubmitJobDirb.cluster	
				构造Cluster对象
					initialize
						这里使用了ServiceLoader技术, 会把所有ClientProtocolProvider的子类加载到frameworkLoader。然后根据配置的mapreduce的框架选择provider，并赋值给clientProtocolProvider(clientProtocolProvider.create赋值给client)。这里配置了yarn, 因此选择YarnClientProtocolProvider。对于YarnClientProtocolProvider的create返回为YarnRunner(conf),赋值给cluster.client字段。
			submit
				构造submitter							//传入的参数分别为cluster.getFileSystem和client.getClient
				submitter.submitJobInternal
					checkSpecs
						检查工作，根据是否有reduce和是否使用新的api检查。这里只考虑使用新的api的情况。
						这里没有设置mapreduce.job.outputformat.class，使用TextOutputFormat
						TextOutputFormat.checkOutputSpecs		//实际使用父类FileOutputFormat的方法
							如果reduce不为0,输出目录为null报错
							如果输出目录存在报错。
					addMRFrameworkToDistributedCache
						检查mapreduce.application.framework.path是否为空。
						不为空，执行分布式缓存相关工作，暂时略。
					设置jobStagingArea							//初始化staging目录					
						最终会调用YarnRunner::getStagingAreaDir => ResourceMgrDelegate::getStagingAreaDir => MRApps::getStagingAreaDir, 然后访问yarn.app.mapreduce.am.staging-dir, 线上集群设置为/user。所以最后为/user/${user}/.staging
					解析当前主机的的ip地址。如果解析到地址就设置submitHostAddress，submitHostName，以及配置项mapreduce.job.submithostname，mapreduce.job.submithostaddress
					submitClient.getNewJobID				//submitClient即cluster.client,因此调用YARNRunner.getNewJobID
						YARNRunner.getNewJobID				//首先向集群提交申请应用，然后生成一个集群号和job号一起组成的jobid
							resMgrDelegate.getNewJobID	//resMgrDelegate的构造函数中已经声明了YarnClient类型的对象client
								client.createApplication.getApplicationSubmissionContext
									createApplication经过一层层调用，最后会RPC调用ApplicationClientProtocol::getNewApplication，向RM申请一个任务。
									appSubmissionContext获取上下文配置
								获取applicationId
								TypeConverter.fromYarn
									获取集群的时间戳，根据集群对应的时间戳和appid生成一个JobId对象
					设置jobid	
					设置submitJobDir								//最后为	/user/${user}/.staging/${appid}
					设置配置参数，mapreduce.job.user.name，hadoop.http.filter.initializers，mapreduce.job.dir
					...token
					设置dfs.erp.authorization等...(JD，暂时略)
					...token
					copyAndConfigureFiles					//第二个参数为submitJobDir
						构造JobResourceUploader
						uploadFiles									//会把指定文件拷贝到hdfs
							获取replication数目，由mapreduce.client.submit.file.replication配置
							判断是否设置mapreduce.client.genericoptionsparser.used，没有设置打印warming
							获取tmpfiles, tmpjars, tmparchives变量的内容						?????变量值?????
							job.getJar							//获取mapreduce.job.jar的内容	?????变量值?????
							检查submitJobDir是否存在，如果已经存在抛出异常。
							创建submitJobDir目录，并设置权限为700
							然后设置${submitJobDir}/files，${submitJobDir}/archives,${submitJobDir}/libjars目录，并将对应的tmpfiles, tmpjars, tmparchives上传到hdfs中。然后将其爱如对应的缓冲。
							同样上传jobjar
							...token
						getWorkingDirectory
							获取mapreduce.job.working.dir的值
					submitJobFile		//得到${submitJobDir}/job.xml
					writeSplits			//返回maps，即整理得到的splits的数目
						writeNewSplits
							获取InputFormat对象input,根据JobContextImpl::getInputFormatClass可知默认使用TextInputFormat。
							input.getSplits		//实际调用FileInputFormat。返回splits，即输入文件对应的切片。
								设置minSize, 为该格式的最小splitsize和该job配置的最小splitsize两者的最小值。前者为1，后者为线上配置为128M。所以为1。
								设置maxSize。根据线上测试为128M。
								listStatus			//遍历输入目录下面的文件，返回文件状态信息到files中
									getInputPaths
										根据mapreduce.input.fileinputformat.inputdir得到输入目录。然后根据,隔离出路径列表。这里暂时只考虑一个路径的情况。
										...token
										根据配置判断是否需要递归判断。这里没有子目录，暂时考虑不需要的情况。
										构造filter，增加hiddenFileFilter(过滤掉以"_"和"."开头的文件)
										然后增加mapreduce.input.pathFilter.class指定的过滤器，这里没有设置。
										根据前面配置的filter配置inputFilter
										根据mapreduce.input.fileinputformat.list-status.num-threads获取线程数numThreads
										singleThreadedListStatus			//返回给result作为listStatus的返回值
											fs.globStatus					//返回符合filter的所有文件对应的FileStatus
											遍历list列表(即传入的输入文件或目录列表)
												假如是目录，就遍历目录，将符合条件的FileStatus放到到result中	
								遍历listStatus返回的输入目录中文件对应的文件状态对象。
									获取文件路径path和长度length									
									获取各个文件对应的块位置blkLocations
									获取文件块大小，默认为128M
									计算splitSize=computeSplitSize	//这里计算应该为128M
									getBlockIndex
									makeSplit
									—————————————————????? 熟悉hdfs再看 ?????————————————————
									
									
									
									根据splitsize构造split，并添加到splits中。
								设置配置项mapreduce.input.fileinputformat.numinputfiles，为文件数
							然后重新排序为数组array
							createSplitFiles				//写一些元数据的信息
					设置mapreduce.job.maps的值，为writeSplits计算得到的map数目
					获取mapreduce.job.queuename为队列名
					获取acl，YARNRunner默认为*,暂时忽略
					...token
					reservation相关，暂时不考虑。
					writeConf										//将配置信息传入hdfs						
					submitClient.submitJob			//submitClient为YarnRunner				
						...token						
						createApplicationSubmissionContext			//传入的jobSubmitDir即前面的submitJobDir
							获取appid
							设置资源。由yarn.app.mapreduce.am.resource.mb为一个container申请的内存。yarn.app.mapreduce.am.resource.cpu-vcores为一个container申请的cpu核数目
							设置jobConfPath为${jobSubmitDir}/job.
							根据jobSubmitDir获取URL格式的yarnUrlForJobSubmitDir
							设置localResources,将job.xml，${mapreduce.job.jar},${job.split},${job.splitmetainfo}等加入到localResources中
							...token
							设置命令vargs，设置一些参数。启动类为org.apache.hadoop.mapreduce.v2.app.MRAppMaster。输出和错误分别重定向为<LOG_DIR>/stdout和<LOG_DIR>/stderr，待重置!
							然后增加环境变量。设置shell，LD_LIBRARY_PATH......
							设置acls，队列(mapreduce.job.queuename)等参数
							如果AM container已经存在，特殊标记，暂不考虑。
							设置labels
							设置priority
						submitApplication			//提交应用
						立即返回判断状态
		monitorAndPrintJob
			不断通过isComplete判断job是否完成，未完成继续休眠指定时间(由mapreduce.client.completion.pollinterval指定)

						
							
2.2 MRAppMaster的分析
main
	Thread.setDefaultUncaughtExceptionHandler				//设置uncaugth的异常
	从环境变量中读取containerIdStr										//应该是启动AM这个container的名称
	从环境变量中读取nodeHostString
	从环境变量中读取nodePortString
	从环境变量中读取nodeHttpPortString
	从环境变量中读取appSubmitTimeStr									// app提交的时间							?????待会看看启动脚本是否设置了这些
	检查工作
	获取applicationAttemptId
	构造MRAppMaster对象																//仅仅设置了一些参数，主要是之前环境变量读出来的那些
	addShutdownHook																	//增加了一个hook，检查外界信号，打印适当的中断信息。暂不深入
	将job.xml读到配置中
	MRWebAppUtil.initialize													// MR的web界面
		setHttpPolicyInYARN
		setHttpPolicyInJHS
	读取环境变量的用户名，并设置到配置中
	initAndStartAppMaster
		...			//token,ugi相关，暂时略
		appMaster.init
			appMaster.serviceInit
				createJobClassLoader											//创建类加载器jobClassLoader
				设置DISPATCHER_EXIT_ON_ERROR_KEY为true			//稍后分析?????
				initJobCredentialsAndUGI									//暂不分析
				构造RunningAppContext对象context
				获取appName
				获取jobId
				根据mapreduce.job.reduces获取reuce的数目numReduceTasks，默认是0
				如果numReduceTasks为0且map会reduce使用新的api，则设置newApiCommitter为true							//?????跟reduce的数目有什么关系?????
				获取user和stagingDir，然后是一写检查工作吧，在一个try{}中，这里暂缓分析
				构造dispatcher
				构造createJobHistoryHandler并注册到dispatcher
				createStagingDirCleaningService构造StagingDirCleaningService对象并注册
				createContainerAllocator										//构造组件containerAllocator
				注册containerAllocator到dispatcher
				如果处于copyHistory阶段，就启动JobHistoryCopyService服务
				如果不是copyHistory阶段阶段，也就是正常的流程
					构造clientService
					构造committerEventHandler
					构造taskAttemptListener
					构造historyService
					构造speculatorEventDispatcher
					构造containerLauncher
		appMaster.start		
			appMaster.serviceStart
				构造amInfos
				processRecovery
					判断如果attempt号为1，不用恢复
					根据配置项yarn.app.mapreduce.am.job.recovery.enable判断是否开启了am的recovery功能，默认为开启。
					获取recoverySupportedByCommitter，判断是否支持recovery，暂时好像不支持
					获取numReduceTasks
					... token
					由于暂时不支持，所有不继续分析	
				构造AMInfo对象amInfo
				createJob					
					构造一个Job，类型为JobImpl
					为dispatcher增加一个JobFinishEvent事件和处理句柄，主要为job完成的处理工作
				然后为之前的AM发送信号						//这里暂时不分析，稍后分析它的作用?????
				然后给当前的am发送JobHistoryEvent事件																			//是JH相关事件发起的标志，稍后分析!!!!!
				发送JobEventType.JOB_INIT事件
				发送SpeculatorEvent事件
				clientService.start
					MRClientService.serviceStart
						构造了一个rpc的server，并启动，这个组件是专MR的AM端专门为client端服务，为client提供查看作业状态，控制作业的接口。
				如果初始化没有失败，会调用startJobs
				startJobs
					发送JobStartEvent事件。
					
					
3. MR的有限状态机及事件处理
截止到这里，简单的流程已经分析完成。接下来从发送事件的角度分析。主要是以下三个事件。
(1) JobHistoryEvent事件
(2) JOB_INIT事件
(3) JobStartEvent事件

3.1 JobHistoryEvent事件处理
	appMaster.serviceStart函数中会发送JobHistoryEvent事件，第二个参数为AMStartedEvent事件
	这里会交给JobHistoryEventHandler.handle处理
	JobHistoryEventHandler.handle
		调整maxUnflushedCompletionEvents的值
		把事件放入eventQueue队列中，在JobHistoryEventHandler.serviceStart中处理，然后调用handleEvent处理事件
		handleEvent
			这里为AM_STARTED事件
			setupEventWriter
			...				//一些jobhistory的处理事件，暂时不分析
			
3.2 JOB_INIT事件处理
	JOB_INIT事件会交给JobEventDispatcher处理
		JobEventDispatcher.handle
			context.getJob(event.getJobId())).handle(event)		//其中context为RunningAppContext
				根据jobid得到对应的job后，调用其handle函数处理事件
				JobImpl.handle
					进入状态机处理。由于初始化Job状态机状态为JobStateInternal.NEW
					JobStateInternal.NEW==(JOB_INIT,InitTransition)==>JobStateInternal.INITED
					InitTransition.transition
						构造JobContextImpl对象
						setup						//一些准备工作，设置目录，token等
						设置job.fs			//这里为hdfs文件系统
						构造JobSubmittedEvent事件，并封装到JobHistoryEvent中并发送				!!!!!继续分析!!!!!
						createSplits		//读取split信息
						得到map和reduce的个数
						checkTaskLimits				//暂时没做任何工作
						计算inputLength，调用makeUberDecision决定是否使用uber模式					//暂不考虑uber模式
						...										//配置一些参数，暂不分析
						createMapTasks
							构造一系列的MapTaskImpl加入到mapTasks中
						createReduceTasks
							构造一系列的ReduceTaskImpl加入到reduceTasks中
						返回JobStateInternal.INITED状态。如果有异常，会维持JobStateInternal.NEW状态。

3.3 JobStartEvent事件
	JobStartEvent事件会交给JobEventDispatcher处理
		JobEventDispatcher.handle
			JobImpl.handle
				JobStateInternal.INITED==(JOB_START,StartTransition)==>JobStateInternal.SETUP
				StartTransition.transition
					如果是恢复的job，就设置starttime为之前的开始时间，否则设置为当前时间。
					构造JobInitedEvent封装到JobHistoryEvent，并发送
					构造JobInfoChangeEvent封装到JobHistoryEvent，并发送
					发送CommitterJobSetupEvent事件
									
	处理CommitterJobSetupEvent事件
		CommitterEventHandler.handle
			添加到eventQueue队列中在serviceStart中处理
			launcherPool.execute										//使用EventProcessor处理
				EventProcessor.run
					进入JOB_SETUP分支，handleJobSetup
					handleJobSetup
						committer.setupJob								//这里为FileOutputCommitter
							构造jobAttemptPath，并创建相应的目录		//hdfs上的目录
						发送JobSetupCompletedEvent事件

	处理JobSetupCompletedEvent事件
		JobStateInternal.SETUP==(JobEventType.JOB_SETUP_COMPLETED,SetupCompletedTransition)==>JobStateInternal.RUNNING,
    SetupCompletedTransition.transition
    	设置job.setupProgress为1
    	job.scheduleTasks(job.mapTasks, job.numReduceTasks == 0)				//对于wordcount，第二个参数必然为false
    		加入有已经完成的task，会发送TaskRecoverEvent				//这里暂时不考虑recovery模式，暂不分析
				如果不是已完成的task，会发送TaskEvent，类型为TaskEventType.T_SCHEDULE
			job.scheduleTasks(job.reduceTasks, true)
				同上。				//这里根据map和reduce的数目发送类型为TaskEventType.T_SCHEDULE的TaskEvent
			如果map和reduce的数目都为0，就发送JOB_COMPLETED事件

	处理类型为TaskEventType.T_SCHEDULE的TaskEventType事件
		TaskEventDispatcher.handle					//注: 下面的task是在createMapTasks和createReduceTasks创建的,对应的类分别为MapTaskImpl和ReduceTaskImpl
			获取收到事件对应的task								//对于map为MapTaskImpl，对于reduce为ReduceTaskImpl,但执行的handle函数实际为父类TaskImpl的handle
			TaskImpl.handle
				进行状态转换，TaskImpl的状态为TaskStateInternal.NEW
					TaskStateInternal.NEW==(TaskEventType.T_SCHEDULE,InitialScheduleTransition)==>TaskStateInternal.SCHEDULED, 
        	InitialScheduleTransition.transition
        		addAndScheduleAttempt
        			addAttempt
        				createAttempt						//构造TaskAttemptImpl对象
        				增加TaskAttemptImpl到attempts中
							将新的Attempt增加到inProgressAttempts
							加入有失败的就发送类型为TaskAttemptEventType.TA_RESCHEDULE的TaskAttemptEvent事件，这里暂时不考虑有失败的情况
							没有失败的发送类型为TaskAttemptEventType.TA_SCHEDULE的TaskAttemptEvent事件
						更新scheduledTime
						sendTaskStartedEvent
							构造TaskStartedEvent封装成JobHistoryEvent，并发送信号					//JH的信号处理暂时不分析

	处理类型为TaskAttemptEventType.TA_SCHEDULE的TaskAttemptEvent的事件
		这里对于TaskAttemptImpl的初始状态为TaskAttemptStateInternal.NEW				
			TaskAttemptStateInternal.NEW==(TaskAttemptEventType.TA_SCHEDULE,RequestContainerTransition(false))==>TaskAttemptStateInternal.UNASSIGNED,
       	RequestContainerTransition.transition
					发送SpeculatorEvent事件
					这里传入的rescheduled为false，所以发送ContainerRequestEvent事件，类型为ContainerAllocator.EventType.CONTAINER_REQ
					
	处理ContainerAllocator.EventType.CONTAINER_REQ类型的ContainerRequestEvent事件
		最终会交给MRAppMaster的组件containerAllocator执行
			ContainerAllocatorRouter.handle		
				containerAllocator.handle							//containerAllocator的类型为RMContainerAllocator
					将事件写入eventQueue队列待serviceStart中循环读取事件，通过handleEvent处理
					handleEvent
						这里仅需要分析CONTAINER_REQ分支
						getMaxContainerCapability							//获取资源最大值，RMContainerAllocator就是一个RMContainerRequestor的子类,调用父类RMCommunicator.getMaxContainerCapability。是与RM通信调用的registerApplicationMaster得到的资源信息(可参见SimpleAppMaster.java)
						如果是map的话，会做一些检查工作，具体暂不分析
						设置memory和cpu核心数
						scheduledRequests.addMap
							这里遍历了host和rack，稍后详细分析这两个参数的来源后再深入?????
							构造ContainerRequest，放入map中
							addContainerReq								//很明显对应于SimpleAppMaster.java的申请资源的过程
								addResourceRequest					
									构造remoteRequest
									addResourceRequestToAsk		//RMContainerRequestor的ask字段是怎么回事
	截止到这里，TaskAttempt向ContainerAllocator发送请求，ContainerAllocator然后先RM申请资源，然后等待，通过心跳查看是否分配到资源。
									

	RMCommunicator.startAllocatorThread会启动心跳服务heartbeat，子类RMContainerAllocator中实现了heartbeat。从这里开始分析:
		getResources					//获取被分配资源的container，稍后需要详细分析?????
		scheduledRequests.assign		
		更新已经完成map和reduce数目等参数
		scheduleReduces			
			...									//一些检查工作
			获取被分配container的地址allocatedHost
			检查是否在黑名单中
			assignContainers
				遍历container
					assignWithoutLocality
						调用assignToFailedMap或assignToReduce，从map和reduce中将ContainerRequest拿去
					containerAssigned						//加入container已经assigned的操作
						发送TaskAttemptContainerAssignedEvent事件
						将分配到container和attemptid写入到assignedRequests
				assignMapsWithLocality
					根据container找到对应的host，然后对host下所有的TaskAttempt操作。调用containerAssigned标记assinged，发送信号。这个步骤实质上就是在同一台机器启动container。发送JobCounterUpdateEvent事件。
					匹配同机架的container
					匹配剩余的																			// 这个过程发生了什么????? container是哪一步与host联系到一块的?????
			释放不能分配的container

	处理TaskAttemptContainerAssignedEvent事件
		前面分析可以知道，containerallocator处理心跳包后分配container的时候，对已经分配的container对会发送TaskAttemptContainerAssignedEvent事件
		TaskAttemptStateInternal.UNASSIGNED==(TaskAttemptEventType.TA_ASSIGNED,ContainerAssignedTransition)==>TaskAttemptStateInternal.ASSIGNED
		ContainerAssignedTransition.transition
			createRemoteTask
			registerPendingTask
			构造上下文
			发送ContainerRemoteLaunchEvent
			发送SpeculatorEvent
			
	处理ContainerRemoteLaunchEvent事件					//猜测是远程启动contaienr，当然也可能是本地
		交给ContainerLauncherRouter处理
		ContainerLauncherRouter.handle
			ContainerLauncherImpl.handle
				将事件放到eventQueue队列中待ContainerLauncherImpl.serviceStart中处理。以下是serviceStart得到处理后的结果					
					allNodes.add(event.getContainerMgrAddress());
					...			//检查工作
					launcherPool.execute(createEventProcessor(event))
						会执行EventProcessor.run
							进入CONTAINER_REMOTE_LAUNCH分支
							c.launch(launchEvent)
								构造startRequest，然后修改为StartContainersRequest格式
								startContainers						//ContainerManagementProtocol协议调用，rpc远程调用实现container的启动。
								根据startContainers返回的reponse构造portInfo，然后得到port
								发送TaskAttemptContainerLaunchedEvent事件，发送的参数是taskAttemptID和port

	处理TaskAttemptContainerLaunchedEvent事件
		TaskAttemptStateInternal.ASSIGNED==(TaskAttemptEventType.TA_CONTAINER_LAUNCHED,LaunchedContainerTransition)==>TaskAttemptStateInternal.RUNNING,
		LaunchedContainerTransition.transition
			设置taskAttempt启动时间、shuffle端口等...
			发送SpeculatorEvent事件
			发送T_ATTEMPT_LAUNCHED类型的TaskTAttemptEvent事件


	处理T_ATTEMPT_LAUNCHED类型的TaskTAttemptEvent事件
	TaskStateInternal.SCHEDULED==(TaskEventType.T_ATTEMPT_LAUNCHED,LaunchTransition)==>TaskStateInternal.RUNNING         
		LaunchTransition.transition
			!!!!!here!!!!!
			
			

		ContainerAllocatorRouter.serviceStart
			这里暂时不考虑uber模式
			构造RMContainerAllocator对象containerAllocator
			RMContainerAllocator.serviceInit
				根据配置项mapreduce.job.reduce.slowstart.completedmaps获取reduceSlowStart，默认0.05
				根据配置项yarn.app.mapreduce.am.job.reduce.rampup.limit获取maxReduceRampupLimit，默认0.5
				根据配置项yarn.app.mapreduce.am.job.reduce.preemption.limit获取maxReducePreemptionLimit，默认0.5
				根据配置项mapreduce.job.reducer.preempt.delay.sec获取allocationDelayThresholdMs，默认0，会乘以1000转化为ms
				根据配置项mapreduce.job.running.map.limit获取maxRunningMaps，默认0
				根据配置项mapreduce.job.running.reduce.limit获取maxRunningReduces，默认0
				RackResolver.init
					获取net.topology.node.switch.mapping.impl的值作为时间类dnsToSwitchMappingClass，默认值为ScriptBasedMapping
					构造dnsToSwitchMappingClass实例
					最后构造得到dnsToSwitchMapping
				根据配置项yarn.app.mapreduce.am.scheduler.connection.wait.interval-ms获取retryInterval，默认360s
				设置当前时间为retrystartTime
			RMContainerAllocator.serviceStart
				循环从eventQueue获取事件,为阻塞调用,当有事件到来,使用handleEvent处理。								//具体的事件处理稍后分析





4. JobHistory说明
使用类: org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer



附录A 配置的说明
mapreduce.client.completion.pollinterval				表示客户端循环判断job是否完成的时间间隔
mapreduce.job.reduces														设置reduce的数目
mapreduce.job.reduce.slowstart.completedmaps

默认的OutputFormat为TextOutputFormat


FileOutputCommitter




附录:
命令:
(1) hdfs dfs -put a.txt /tmp/in						注:这里txt为653M左右
(2) hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar wordcount /tmp/in /tmp/out

集群threshold参数默认为1，maxassign为1
部分命令的打印信息
[jd_ad@BJYF-Druid-15221 ~]$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar wordcount /tmp/in /tmp/out
Not a valid JAR: /home/jd_ad/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar
[jd_ad@BJYF-Druid-15221 ~]$ cd /software/servers/hadoop-2.7.1/
[jd_ad@BJYF-Druid-15221 hadoop-2.7.1]$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar wordcount /tmp/in /tmp/out
[2016-10-27T17:31:57.847+08:00] [INFO] yarn.client.ConfiguredRMFailoverProxyProvider.performFailover(ConfiguredRMFailoverProxyProvider.java 100) [main] : Failing over to rm2
[2016-10-27T17:31:58.504+08:00] [INFO] lib.input.FileInputFormat.listStatus(FileInputFormat.java 283) [main] : Total input paths to process : 1
[2016-10-27T17:31:58.518+08:00] [INFO] compression.lzo.GPLNativeCodeLoader.<clinit>(GPLNativeCodeLoader.java 52) [main] : Loaded native gpl library from the embedded binaries
[2016-10-27T17:31:58.520+08:00] [INFO] compression.lzo.LzoCodec.<clinit>(LzoCodec.java 76) [main] : Successfully loaded & initialized native-lzo library [hadoop-lzo rev 40b332285be4d4a9ec95a02694b20211810cf4ec]
[2016-10-27T17:31:58.839+08:00] [INFO] hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java 233) [main] : number of splits:3
[2016-10-27T17:31:59.081+08:00] [INFO] hadoop.mapreduce.JobSubmitter.printTokens(JobSubmitter.java 322) [main] : Submitting tokens for job: job_1477447398282_0058
[2016-10-27T17:31:59.441+08:00] [INFO] api.impl.YarnClientImpl.submitApplication(YarnClientImpl.java 273) [main] : Submitted application application_1477447398282_0058
[2016-10-27T17:31:59.493+08:00] [INFO] hadoop.mapreduce.Job.submit(Job.java 1294) [main] : The url to track the job: http://BJYF-Druid-15216.hadoop.jd.local:50320/proxy/application_1477447398282_0058/
[2016-10-27T17:31:59.494+08:00] [INFO] hadoop.mapreduce.Job.monitorAndPrintJob(Job.java 1339) [main] : Running job: job_1477447398282_0058
[2016-10-27T17:32:16.739+08:00] [INFO] hadoop.mapreduce.Job.monitorAndPrintJob(Job.java 1360) [main] : Job job_1477447398282_0058 running in uber mode : false
[2016-10-27T17:32:16.742+08:00] [INFO] hadoop.mapreduce.Job.monitorAndPrintJob(Job.java 1367) [main] :  map 0% reduce 0%

map的信息
attempt_1477447398282_0058_m_000000_0	SUCCEEDED	map	/default-rack/BJYF-Druid-15219.hadoop.jd.local:8042	logs	Thu Oct 27 17:32:17 +0800 2016	Thu Oct 27 17:32:52 +0800 2016	34sec	
attempt_1477447398282_0058_m_000001_0	SUCCEEDED	map	/default-rack/BJYF-Druid-18531.hadoop.jd.local:8042	logs	Thu Oct 27 17:32:17 +0800 2016	Thu Oct 27 17:32:52 +0800 2016	34sec	
attempt_1477447398282_0058_m_000002_0	SUCCEEDED	map	/default-rack/BJYF-Druid-184126.hadoop.jd.local:8042	logs	Thu Oct 27 17:32:17 +0800 2016	Thu Oct 27 17:32:43 +0800 2016	25sec

reduce的信息
attempt_1477447398282_0058_r_000000_0	SUCCEEDED	reduce > reduce	/default-rack/BJYF-Druid-184122.hadoop.jd.local:8042	logs	Thu Oct 27 17:32:54 +0800 2016	Thu Oct 27 17:33:07 +0800 2016	Thu Oct 27 17:33:08 +0800 2016	Thu Oct 27 17:33:11 +0800 2016	12sec	1sec	3sec	16sec	

fsck信息
hdfs fsck /tmp/in/a.txt -files -locations -blocks   
Connecting to namenode via http://BJYF-Druid-15216.hadoop.jd.local:50070/fsck?ugi=jd_ad&files=1&locations=1&blocks=1&path=%2Ftmp%2Fin%2Fa.txt
FSCK started by jd_ad (auth:SIMPLE) from /172.19.152.21 for path /tmp/in/a.txt at Thu Oct 27 17:41:25 CST 2016
/tmp/in/a.txt 684509280 bytes, 6 block(s):  OK
0. BP-1089191954-172.19.152.15-1474462508760:blk_1073879805_141254 len=134217728 repl=3 [DatanodeInfoWithStorage[172.19.152.19:50010,DS-e12c3e4e-ca10-4633-8c22-f2398fe198f7,DISK], DatanodeInfoWithStorage[172.16.184.120:50010,DS-cb0a8ae4-255c-4bdf-8cf8-3c7066d6dc61,DISK], DatanodeInfoWithStorage[172.16.184.126:50010,DS-5045618b-3107-4ebe-856d-ae1b25710258,DISK]]
1. BP-1089191954-172.19.152.15-1474462508760:blk_1073879806_141255 len=134217728 repl=3 [DatanodeInfoWithStorage[172.16.185.33:50010,DS-b09bb078-d82a-4c3d-b37a-e09e25f9fbac,DISK], DatanodeInfoWithStorage[172.19.152.20:50010,DS-70c45b7a-0e6c-4390-9368-2423f558eb6a,DISK], DatanodeInfoWithStorage[172.16.185.23:50010,DS-48ee9c8d-8d11-4445-8623-8f3482b34897,DISK]]
2. BP-1089191954-172.19.152.15-1474462508760:blk_1073879807_141256 len=134217728 repl=3 [DatanodeInfoWithStorage[172.19.152.19:50010,DS-2f6ea0c2-2ff0-4c27-a249-8da8fda15678,DISK], DatanodeInfoWithStorage[172.16.185.31:50010,DS-0b387221-c615-4bd0-92ab-3ab82e2e58cd,DISK], DatanodeInfoWithStorage[172.16.184.122:50010,DS-c6024932-c060-445e-86cf-8fbdd772a08b,DISK]]
3. BP-1089191954-172.19.152.15-1474462508760:blk_1073879808_141257 len=134217728 repl=3 [DatanodeInfoWithStorage[172.16.184.130:50010,DS-33115c48-e357-409a-bb70-f99cec24d1f4,DISK], DatanodeInfoWithStorage[172.16.185.21:50010,DS-57e8f88f-a98e-4257-badf-791d05d67426,DISK], DatanodeInfoWithStorage[172.16.184.110:50010,DS-b524823d-056a-4fbb-aca5-8afd09efd4c5,DISK]]
4. BP-1089191954-172.19.152.15-1474462508760:blk_1073879809_141258 len=134217728 repl=3 [DatanodeInfoWithStorage[172.16.184.120:50010,DS-dfa93948-94bd-43b0-998b-53beaf0ce862,DISK], DatanodeInfoWithStorage[172.16.184.110:50010,DS-af3a7f78-e772-455a-8106-a5c57778646e,DISK], DatanodeInfoWithStorage[172.16.184.124:50010,DS-1db86050-d376-4596-b318-7521b2b7fa3d,DISK]]
5. BP-1089191954-172.19.152.15-1474462508760:blk_1073879810_141259 len=13420640 repl=3 [DatanodeInfoWithStorage[172.16.185.33:50010,DS-7e31ce5f-66e1-49a0-bdd0-a854a459490e,DISK], DatanodeInfoWithStorage[172.16.185.21:50010,DS-5d3d13a8-65ee-4377-9f32-b76ff90ec93a,DISK], DatanodeInfoWithStorage[172.16.185.29:50010,DS-ad4aa826-1a68-4297-9c4d-cd4bff1284e0,DISK]]
Status: HEALTHY
 Total size:    684509280 B
 Total dirs:    0
 Total files:   1
 Total symlinks:                0
 Total blocks (validated):      6 (avg. block size 114084880 B)
 Minimally replicated blocks:   6 (100.0 %)
 Over-replicated blocks:        0 (0.0 %)
 Under-replicated blocks:       0 (0.0 %)
 Mis-replicated blocks:         0 (0.0 %)
 Default replication factor:    3
 Average block replication:     3.0
 Corrupt blocks:                0
 Missing replicas:              0 (0.0 %)
 Number of data-nodes:          28
 Number of racks:               1
FSCK ended at Thu Oct 27 17:41:25 CST 2016 in 1 milliseconds
The filesystem under path '/tmp/in/a.txt' is HEALTHY



hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar wordcount -Dyarn.app.mapreduce.am.resource.cpu-vcores=10 /tmp/in /tmp/out
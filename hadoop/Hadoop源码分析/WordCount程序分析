		一个MapReduce程序的分析
注: mapreduce新版api, mapred是旧版api

本文以WordCount为例进行分析。
1. wordcount任务的执行
(1) hdfs dfs -put a.txt /tmp/in						注:这里txt为653M左右
(2) hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar wordcount /tmp/in /tmp/out
2. 源码分析
2.1 wordcount程序的提交
WordCount::main
	GenericOptionsParser					//分析传入参数，这里传入的参数只有两个: /tmp/in /tmp/out	
	Job.getInstance
	GenericOptionsParser					//处理传入参数加入到conf中
	Job.getInstance							//构造一个Job对象，继承于JobContextImpl
		getInstance
			构造Job对象
				设置Job的cluster字段为null
		setJobName				
	job.setJarByClass								
	job.setMapperClass
	job.setCombinerClass
	job.setReducerClass
	job.setOutputKeyClass
	job.setOutputValueClass					//以上是一些设置，设置到了jobconf中
	FileInputFormat.addInputPath
	FileOutputFormat.setOutputPath
	waitForCompletion						//org.apache.hadoop.mapreduce.Job.waitForCompletion
		submit										//只要为DEFINE状态才提交
			setUseNewAPI						
				设置numReduces，由mapreduce.job.reduces设置，默认为1				
				假如没有配置mapred.mapper.class就设置mapred.mapper.new-api为true，标记使用新的map的api
				假如没有配置mapred.reducer.class就设置mapred.reducer.new-api为true，标记使用新的reduce的api
			connect									//构造josubmitJobDirb.cluster	
				构造Cluster对象
					initialize
						这里使用了ServiceLoader技术, 会把所有ClientProtocolProvider的子类加载到frameworkLoader。然后根据配置的mapreduce的框架选择provider，并赋值给clientProtocolProvider(clientProtocolProvider.create赋值给client)。这里配置了yarn, 因此选择YarnClientProtocolProvider。对于YarnClientProtocolProvider的create返回为YarnRunner(conf),赋值给cluster.client字段。
			submit
				构造submitter							//传入的参数分别为cluster.getFileSystem和client.getClient
				submitter.submitJobInternal
					checkSpecs
						检查工作，根据是否有reduce和是否使用新的api检查。这里只考虑使用新的api的情况。
						这里没有设置mapreduce.job.outputformat.class，使用TextOutputFormat
						TextOutputFormat.checkOutputSpecs		//实际使用父类FileOutputFormat的方法
							如果reduce不为0,输出目录为null报错
							如果输出目录存在报错。
					addMRFrameworkToDistributedCache
						检查mapreduce.application.framework.path是否为空。
						不为空，执行分布式缓存相关工作，暂时略。
					设置jobStagingArea							//初始化staging目录					
						最终会调用YarnRunner::getStagingAreaDir => ResourceMgrDelegate::getStagingAreaDir => MRApps::getStagingAreaDir, 然后访问yarn.app.mapreduce.am.staging-dir, 线上集群设置为/user。所以最后为/user/${user}/.staging
					解析当前主机的的ip地址。如果解析到地址就设置submitHostAddress，submitHostName，以及配置项mapreduce.job.submithostname，mapreduce.job.submithostaddress
					submitClient.getNewJobID				//submitClient即cluster.client,因此调用YARNRunner.getNewJobID
						YARNRunner.getNewJobID				//首先向集群提交申请应用，然后生成一个集群号和job号一起组成的jobid
							resMgrDelegate.getNewJobID	//resMgrDelegate的构造函数中已经声明了YarnClient类型的对象client
								client.createApplication.getApplicationSubmissionContext
									createApplication经过一层层调用，最后会RPC调用ApplicationClientProtocol::getNewApplication，向RM申请一个任务。
									appSubmissionContext获取上下文配置
								获取applicationId
								TypeConverter.fromYarn
									获取集群的时间戳，根据集群对应的时间戳和appid生成一个JobId对象
					设置jobid	
					设置submitJobDir								//最后为	/user/${user}/.staging/${appid}
					设置配置参数，mapreduce.job.user.name，hadoop.http.filter.initializers，mapreduce.job.dir
					...token
					设置dfs.erp.authorization等...(JD，暂时略)
					...token
					copyAndConfigureFiles					//第二个参数为submitJobDir
						构造JobResourceUploader
						uploadFiles									//会把指定文件拷贝到hdfs
							获取replication数目，由mapreduce.client.submit.file.replication配置
							判断是否设置mapreduce.client.genericoptionsparser.used，没有设置打印warming
							获取tmpfiles, tmpjars, tmparchives变量的内容						?????变量值?????
							job.getJar							//获取mapreduce.job.jar的内容	?????变量值?????
							检查submitJobDir是否存在，如果已经存在抛出异常。
							创建submitJobDir目录，并设置权限为700
							然后设置${submitJobDir}/files，${submitJobDir}/archives,${submitJobDir}/libjars目录，并将对应的tmpfiles, tmpjars, tmparchives上传到hdfs中。然后将其爱如对应的缓冲。
							同样上传jobjar
							...token
						getWorkingDirectory
							获取mapreduce.job.working.dir的值
					submitJobFile		//得到${submitJobDir}/job.xml
					writeSplits			//返回maps，即整理得到的splits的数目
						writeNewSplits
							获取InputFormat对象input,根据JobContextImpl::getInputFormatClass可知默认使用TextInputFormat。
							input.getSplits		//实际调用FileInputFormat。返回splits，即输入文件对应的切片。
								设置minSize, 为该格式的最小splitsize和该job配置的最小splitsize两者的最小值。前者为1，后者为线上配置为128M。所以为1。
								设置maxSize。根据线上测试为128M。
								listStatus			//遍历输入目录下面的文件，返回文件状态信息到files中
									getInputPaths
										根据mapreduce.lib.input.FileInputFormat.inputdir得到输入目录。然后根据,隔离出路径列表。这里暂时只考虑一个路径的情况。
										...token
										根据配置判断是否需要递归判断。这里没有子目录，暂时考虑不需要的情况。
										构造filter，增加hiddenFileFilter(过滤掉以"_"和"."开头的文件)
										然后增加mapreduce.input.pathFilter.class指定的过滤器，这里没有设置。
										根据前面配置的filter配置inputFilter
										根据mapreduce.lib.input.FileInputFormat.list-status.num-threads获取线程数numThreads
										singleThreadedListStatus			//返回给result作为listStatus的返回值
											fs.globStatus					//返回符合filter的所有文件对应的FileStatus
											遍历list列表(即传入的输入文件或目录列表)
												假如是目录，就遍历目录，将符合条件的FileStatus放到到result中	
								遍历listStatus返回的输入目录中文件对应的文件状态对象。
									获取文件路径path和长度length									
									获取各个文件对应的块位置blkLocations
									获取文件块大小，默认为128M
									计算splitSize=computeSplitSize	//这里计算应该为128M
									getBlockIndex
									makeSplit
									—————————————————????? 熟悉hdfs再看 ?????————————————————									
									根据splitsize构造split，并添加到splits中。
								设置配置项mapreduce.lib.input.FileInputFormat.numinputfiles，为文件数
							然后重新排序为数组array
							createSplitFiles				//写一些元数据的信息
					设置mapreduce.job.maps的值，为writeSplits计算得到的map数目
					获取mapreduce.job.queuename为队列名
					获取acl，YARNRunner默认为*,暂时忽略
					...token
					reservation相关，暂时不考虑。
					writeConf										//将配置信息传入hdfs						
					submitClient.submitJob			//submitClient为YarnRunner				
						...token						
						createApplicationSubmissionContext			//传入的jobSubmitDir即前面的submitJobDir
							获取appid
							设置资源。由yarn.app.mapreduce.am.resource.mb为一个container申请的内存。yarn.app.mapreduce.am.resource.cpu-vcores为一个container申请的cpu核数目
							设置jobConfPath为${jobSubmitDir}/job.
							根据jobSubmitDir获取URL格式的yarnUrlForJobSubmitDir
							设置localResources,将job.xml，${mapreduce.job.jar},${job.split},${job.splitmetainfo}等加入到localResources中
							...token
							设置命令vargs，设置一些参数。启动类为org.apache.hadoop.mapreduce.v2.app.MRAppMaster。输出和错误分别重定向为<LOG_DIR>/stdout和<LOG_DIR>/stderr，待重置!
							然后增加环境变量。设置shell，LD_LIBRARY_PATH......
							设置acls，队列(mapreduce.job.queuename)等参数
							如果AM container已经存在，特殊标记，暂不考虑。
							设置labels
							设置priority
						submitApplication			//提交应用
						立即返回判断状态
		monitorAndPrintJob
			不断通过isComplete判断job是否完成，未完成继续休眠指定时间(由mapreduce.client.completion.pollinterval指定)

						
							
2.2 MRAppMaster的分析
main
	Thread.setDefaultUncaughtExceptionHandler				//设置uncaugth的异常
	从环境变量中读取containerIdStr										//应该是启动AM这个container的名称
	从环境变量中读取nodeHostString
	从环境变量中读取nodePortString
	从环境变量中读取nodeHttpPortString
	从环境变量中读取appSubmitTimeStr									// app提交的时间							?????待会看看启动脚本是否设置了这些
	检查工作
	获取applicationAttemptId
	构造MRAppMaster对象																//仅仅设置了一些参数，主要是之前环境变量读出来的那些
	addShutdownHook																	//增加了一个hook，检查外界信号，打印适当的中断信息。暂不深入
	将job.xml读到配置中
	MRWebAppUtil.initialize													// MR的web界面
		setHttpPolicyInYARN
		setHttpPolicyInJHS
	读取环境变量的用户名，并设置到配置中
	initAndStartAppMaster
		...			//token,ugi相关，暂时略
		appMaster.init
			appMaster.serviceInit
				createJobClassLoader											//创建类加载器jobClassLoader
				设置DISPATCHER_EXIT_ON_ERROR_KEY为true			//稍后分析?????
				initJobCredentialsAndUGI									//暂不分析
				构造RunningAppContext对象context
				获取appName
				获取jobId
				根据mapreduce.job.reduces获取reuce的数目numReduceTasks，默认是0
				如果numReduceTasks为0且map会reduce使用新的api，则设置newApiCommitter为true							//?????跟reduce的数目有什么关系?????
				获取user和stagingDir，然后是一写检查工作吧，在一个try{}中，这里暂缓分析
				构造dispatcher
				构造createJobHistoryHandler并注册到dispatcher
				createStagingDirCleaningService构造StagingDirCleaningService对象并注册
				createContainerAllocator										//构造组件containerAllocator
				注册containerAllocator到dispatcher
				如果处于copyHistory阶段，就启动JobHistoryCopyService服务
				如果不是copyHistory阶段阶段，也就是正常的流程
					构造clientService
					构造committerEventHandler
					构造taskAttemptListener
					构造historyService
					构造speculatorEventDispatcher
					构造containerLauncher
		appMaster.start		
			appMaster.serviceStart
				构造amInfos
				processRecovery
					判断如果attempt号为1，不用恢复
					根据配置项yarn.app.mapreduce.am.job.recovery.enable判断是否开启了am的recovery功能，默认为开启。
					获取recoverySupportedByCommitter，判断是否支持recovery，暂时好像不支持
					获取numReduceTasks
					... token
					由于暂时不支持，所有不继续分析	
				构造AMInfo对象amInfo
				createJob					
					构造一个Job，类型为JobImpl
					为dispatcher增加一个JobFinishEvent事件和处理句柄，主要为job完成的处理工作
				然后为之前的AM发送信号						//刚开始程序所以amInfos为空，所以这里暂时不分析，稍后分析它的作用?????
				然后给当前的am发送JobHistoryEvent事件																			//是JH相关事件发起的标志，稍后分析!!!!!
				发送JobEventType.JOB_INIT事件
				发送SpeculatorEvent事件
				clientService.start
					MRClientService.serviceStart
						构造了一个rpc的server，并启动，这个组件是专MR的AM端专门为client端服务，为client提供查看作业状态，控制作业的接口。
				如果初始化没有失败，会调用startJobs
				startJobs
					发送JobStartEvent事件。			
					
3. MR的有限状态机及事件处理
截止到这里，简单的流程已经分析完成。接下来从发送事件的角度分析。主要是以下三个事件。
(1) JobHistoryEvent事件							//根据appMaster.init的注册可以知道JobHistoryEvent事件会交给JobHistoryEventHandler处理。
(2) JOB_INIT事件
(3) JobStartEvent事件

3.1 JobHistoryEvent事件处理
	appMaster.serviceStart函数中会发送JobHistoryEvent事件，第二个参数为AMStartedEvent事件
	这里会交给JobHistoryEventHandler.handle处理
	JobHistoryEventHandler.handle
		调整maxUnflushedCompletionEvents的值
		把事件放入eventQueue队列中，在JobHistoryEventHandler.serviceStart中处理，然后调用handleEvent处理事件
		handleEvent
			这里为AM_STARTED事件
			setupEventWriter
			...				//一些jobhistory的处理事件，暂时不分析
			
3.2 JOB_INIT事件处理
	JOB_INIT事件会交给JobEventDispatcher处理
		JobEventDispatcher.handle
			context.getJob(event.getJobId())).handle(event)		//其中context为RunningAppContext
				根据jobid得到对应的job后，调用其handle函数处理事件
				JobImpl.handle
					进入状态机处理。由于初始化Job状态机状态为JobStateInternal.NEW
					JobStateInternal.NEW==(JOB_INIT,InitTransition)==>JobStateInternal.INITED
					InitTransition.transition
						构造JobContextImpl对象
						setup						//一些准备工作，设置目录，token等
						设置job.fs			//这里为hdfs文件系统
						构造JobSubmittedEvent事件，并封装到JobHistoryEvent中并发送				!!!!!继续分析!!!!!
						createSplits		//读取split信息
						得到map和reduce的个数
						checkTaskLimits				//暂时没做任何工作
						计算inputLength，调用makeUberDecision决定是否使用uber模式					//暂不考虑uber模式
						...										//配置一些参数，暂不分析
						createMapTasks
							构造一系列的MapTaskImpl加入到mapTasks中
						createReduceTasks
							构造一系列的ReduceTaskImpl加入到reduceTasks中
						返回JobStateInternal.INITED状态。如果有异常，会维持JobStateInternal.NEW状态。

3.3 JobStartEvent事件
	JobStartEvent事件会交给JobEventDispatcher处理
		JobEventDispatcher.handle
			JobImpl.handle
				JobStateInternal.INITED==(JOB_START,StartTransition)==>JobStateInternal.SETUP
				StartTransition.transition
					如果是恢复的job，就设置starttime为之前的开始时间，否则设置为当前时间。
					构造JobInitedEvent封装到JobHistoryEvent，并发送
					构造JobInfoChangeEvent封装到JobHistoryEvent，并发送
					发送CommitterJobSetupEvent事件
									
	处理CommitterJobSetupEvent事件
		CommitterEventHandler.handle
			添加到eventQueue队列中在serviceStart中处理
			launcherPool.execute										//使用EventProcessor处理
				EventProcessor.run
					进入JOB_SETUP分支，handleJobSetup
					handleJobSetup
						committer.setupJob								//这里为FileOutputCommitter
							构造jobAttemptPath，并创建相应的目录		//hdfs上的目录
						发送JobSetupCompletedEvent事件

	处理JobSetupCompletedEvent事件
		JobStateInternal.SETUP==(JobEventType.JOB_SETUP_COMPLETED,SetupCompletedTransition)==>JobStateInternal.RUNNING,
    SetupCompletedTransition.transition
    	设置job.setupProgress为1
    	job.scheduleTasks(job.mapTasks, job.numReduceTasks == 0)				//对于wordcount，第二个参数必然为false。 这里的mapTask和reduceTask构造于InitTransition中。
    		加入有已经完成的task，会发送TaskRecoverEvent				//这里暂时不考虑recovery模式，暂不分析
				如果不是已完成的task，会发送TaskEvent，类型为TaskEventType.T_SCHEDULE
			job.scheduleTasks(job.reduceTasks, true)
				同上。				//这里根据map和reduce的数目发送类型为TaskEventType.T_SCHEDULE的TaskEvent
			如果map和reduce的数目都为0，就发送JOB_COMPLETED事件

	处理类型为TaskEventType.T_SCHEDULE的TaskEventType事件
		TaskEventDispatcher.handle					//注: 下面的task是在createMapTasks和createReduceTasks创建的,对应的类分别为MapTaskImpl和ReduceTaskImpl
			获取收到事件对应的task								//对于map为MapTaskImpl，对于reduce为ReduceTaskImpl,但执行的handle函数实际为父类TaskImpl的handle
			TaskImpl.handle
				进行状态转换，TaskImpl的状态为TaskStateInternal.NEW
					TaskStateInternal.NEW==(TaskEventType.T_SCHEDULE,InitialScheduleTransition)==>TaskStateInternal.SCHEDULED, 
        	InitialScheduleTransition.transition
        		addAndScheduleAttempt
        			addAttempt
        				createAttempt						//构造TaskAttemptImpl对象
        				增加TaskAttemptImpl到attempts中
							将新的Attempt增加到inProgressAttempts
							加入有失败的就发送类型为TaskAttemptEventType.TA_RESCHEDULE的TaskAttemptEvent事件
							没有失败的发送类型为TaskAttemptEventType.TA_SCHEDULE的TaskAttemptEvent事件
						更新scheduledTime
						sendTaskStartedEvent
							构造TaskStartedEvent封装成JobHistoryEvent，并发送信号					//JH的信号处理暂时不分析

	处理类型为TaskAttemptEventType.TA_SCHEDULE的TaskAttemptEvent的事件
		这里对于TaskAttemptImpl的初始状态为TaskAttemptStateInternal.NEW				
			TaskAttemptStateInternal.NEW==(TaskAttemptEventType.TA_SCHEDULE,RequestContainerTransition(false))==>TaskAttemptStateInternal.UNASSIGNED,
       	RequestContainerTransition.transition
					发送SpeculatorEvent事件,通知该attempt正在请求一个container
					这里传入的rescheduled为false，所以发送ContainerRequestEvent事件，类型为ContainerAllocator.EventType.CONTAINER_REQ
					//注: ContainerRequestEvent传入的四个参数，第一个是attemptid。第二个是申请的资源(根据配置设置)。
					//第三个参数为节点信息，在TaskAttemptImpl的构造函数中调用resolveHosts分析得到要分配的地址，地址列表来自于createSplits通过构造MapTaskImpl传入。值得注意的是这里传入的是一个数组，是一组地址。
					//第四个参数为机架信息，为第三个参数解析成为机架信息的
					
	处理ContainerAllocator.EventType.CONTAINER_REQ类型的ContainerRequestEvent事件
		最终会交给MRAppMaster的组件containerAllocator执行
			ContainerAllocatorRouter.handle		
				containerAllocator.handle							//containerAllocator的类型为RMContainerAllocator
					将事件写入eventQueue队列待serviceStart中循环读取事件，通过handleEvent处理
					handleEvent
						这里仅需要分析为CONTAINER_REQ的if分支
						getMaxContainerCapability							//获取资源最大值，RMContainerAllocator就是一个RMContainerRequestor的子类,调用父类RMCommunicator.getMaxContainerCapability。是与RM通信调用的registerApplicationMaster得到的资源信息(可参见SimpleAppMaster.java)
						如果是map的话
							检查之前是否有设置mapResourceRequest请求，没设置的话就设置一下。				//这里mapResourceRequest是一个引用，为map申请资源的模板，说明一个job请求的每个job都应该是相同的。同reduce
							将传入的事件reqEvent设置memory和cpu核心数
							scheduledRequests.addMap						//这里将设置的reqEvent传入。为scheduledRequests增加一个map的请求
								假如失败过
									将这个attempid加入到earlierFailedMaps，用比较高的优先级构造这个map的请求
								假如第一次
									这里遍历了host(即这个块的所有副本地址)，将host与taskattemptid的对应关系放到mapsHostMapping中
									这里遍历了rack(即这个块的所有副本地址对应的机架信息)，将host与taskattemptid的对应关系放到mapsRackMapping中
									构造ContainerRequest，放入map中
									addContainerReq								//很明显对应于SimpleAppMaster.java的申请资源的过程。这里调用RMContainerRequestor.addContainerReq,RMContainerRequestor是RMContainerAllocator的父类
										addResourceRequest					//值得注意的是，这里增加了三种类别的资源请求。首先会遍历所有host构造请求，然后遍历所有rack构造请求，最后构造一个off-switch级别的请求。一个map的请求的次数为文件副本数+文件副本对应的rack数目+1(off-switch)
											构造remoteRequest加入到remoteRequestsTable字段中。remoteRequestsTable的结构为<优先级<资源名称(节点地址或机架地址或*)<资源(优先级，名称，大小等)，请求>>>
											addResourceRequestToAsk
												将remoteRequest(即remoteRequestsTable的最后一级的value)加入到ask字段中											//这里处理map(reduce之后也是这样处理)的请求，会将请求构造给ask字段
						如果是reduce的话
								......
						
	截止到这里，TaskAttempt向ContainerAllocator发送请求，ContainerAllocator然后先RM申请资源，然后等待，通过心跳查看是否分配到资源。
																											
	RMCommunicator.startAllocatorThread会启动心跳服务heartbeat，子类RMContainerAllocator中实现了heartbeat。从这里开始分析:
		getResources							//返回allocated的container
			applyConcurrentTaskLimits									//配置限制?????
			获取headRoom
			makeRemoteRequest					//会返回response
				applyRequestLimits											//?????
				构造blacklistRequest
				构造allocateRequest
				scheduler.allocate(allocateRequest)			//rpc调用ApplicationMasterService.allocate					//详细看后面的分析
				得到上一次心跳的id，可用资源，集群可用NM的数目，完成的container等信息
				然后清空ask, release, blacklistAdditions, blacklistRemovals列表
			重新获取headroom
			response.getAllocatedContainers						//从返回的response中获取allocatedcontainer
			response.getCompletedContainersStatuses		//从返回的response中获取完成的container的状态
			computeIgnoreBlacklisting
			handleUpdatedNodes
			遍历finishedContainers
				根据containerid获取从assignedRequests获取对应的taskattemptid
				将pendingRelease和assignedRequests将对应这个taskattempt的数据结构移除
				向taskattempt发送createContainerFinishedEvent事件
				发送TaskAttemptDiagnosticsUpdateEvent事件
			最后返回分配的container
		scheduledRequests.assign
			更新containersAllocated
			遍历从RM返回的已经为该app分配的allocatedContainers
				获取分配给该app的container的优先级和可分配的资源量
				如果是map的话，会根据可分配的资源和单个map的资源计算可分配的map的数目。如果小于0，表示无法分配。
				如果是reduc的话，执行与上面类似的工作。
				如果不能分配该container，则执行释放操作。
				如果节点在黑名单中，重新构造map或reduce，并添加请求。
				assignContainers								//之前已经对不满足的条件过滤了，这里执行分配
					遍历之前过滤过的allocatedContainers
						assignWithoutLocality					//首先调用该函数分配没有本地性要求的container
							assignToFailedMap或assignToReduce				//没有本地性要求的container为failedmap何reduce，这里实际主要是将其从map或reduce列表中移除，并返回对应的ContainerRequest结构体
						containerAssigned
							decContainerReq
							发送TaskAttemptContainerAssignedEvent
							将该请求和对应的attemptid加入到assignedRequests中。
						然后将该request从allocatedContainers移除
					assignMapsWithLocality					//这里参数是移除了没有本地性要求的contaienr，因此这里剩下的都是正常的map了
						遍历allocatedContainers
							获取分配的container对象allocated的host
							根据host和mapsHostMapping查找出host对应的attemptid
							拿出attemptid，执行containerAssigned(具体同前)，然后发送JobCounterUpdateEvent事件，同时从allocatedContainers删除。
							然后分配rack匹配的操作，具体类似上面的操作。最后匹配off-switch级别的。当然，至于后面步骤的执行，需要看前面allocatedContainers是否已经被完全分配出去。
				遍历剩余的allocatedContainers以释放
					containerNotAssigned
						更新containersReleased，pendingRelease以及release							
		更新已经完成map和reduce数目等参数
		scheduleReduces					//判断是否需要执行reduce，调度reduce的操作，暂时略
			... 		//待分析
			
						
根据startAllocatorThread进程的心跳服务heartbeat的分析，可知am的心跳从rm中获取分配的container之后，分给指定的map之后会发送TaskAttemptContainerAssignedEvent事件。所以，这里回到TaskAttemptStateInternal状态机继续分析。
	处理TaskAttemptContainerAssignedEvent事件
		前面分析可以知道，containerallocator处理心跳包后分配container的时候，对已经分配的container对会发送TaskAttemptContainerAssignedEvent事件
		TaskAttemptStateInternal.UNASSIGNED==(TaskAttemptEventType.TA_ASSIGNED,ContainerAssignedTransition)==>TaskAttemptStateInternal.ASSIGNED
		ContainerAssignedTransition.transition
			设置taskAttempt的container字段
			设置taskAttempt的remoteTask字段，会调用createRemoteTask。对于map是会构造MapTask。
			设置taskAttempt的jvmID字段
			registerPendingTask							//做了什么?????有什么用?????
			computeRackAndLocality					//得到当前attempt的本地性等级			
			构造launchContext上下文
			发送ContainerRemoteLaunchEvent
			发送SpeculatorEvent

MRAppMaster中注册了containerLauncher，这里仅考虑远程启动contaienr，即考虑类ContainerLauncherImpl而不是LocalContainerLauncher。
因此，处理ContainerRemoteLaunchEvent事件交给MRAppMaster.containerLauncher处理，具体实现是ContainerLauncherImpl.handle处理
	处理ContainerRemoteLaunchEvent事件					
		交给ContainerLauncherRouter处理
		ContainerLauncherRouter.handle
			ContainerLauncherImpl.handle
				将事件放到eventQueue队列中待ContainerLauncherImpl.serviceStart中处理。以下是serviceStart得到处理后的结果					
					allNodes.add(event.getContainerMgrAddress());
					...			//检查工作
					launcherPool.execute(createEventProcessor(event))
						会执行EventProcessor.run
							进入CONTAINER_REMOTE_LAUNCH分支
							c.launch(launchEvent)
								构造startRequest，然后修改为StartContainersRequest格式
								startContainers						//ContainerManagementProtocol协议调用，rpc远程调用实现container的启动。			具体的条用过程，详见"yarn应用程序的启动"
								根据startContainers返回的reponse构造portInfo，然后得到port
								发送TaskAttemptContainerLaunchedEvent事件，发送的参数是taskAttemptID和port

	处理TaskAttemptContainerLaunchedEvent事件
		TaskAttemptStateInternal.ASSIGNED==(TaskAttemptEventType.TA_CONTAINER_LAUNCHED,LaunchedContainerTransition)==>TaskAttemptStateInternal.RUNNING,
		LaunchedContainerTransition.transition
			设置taskAttempt启动时间, shuffle端口, trackerName, httpPort等...
			taskAttemptListener.registerLaunchedTask					//会将当前app的信息注册到MRAppMaster.taskAttemptListener, 其中taskAttemptListener是TaskAttemptListenerImpl，完成了TaskUmbilicalProtocol和TaskAttemptListener接口。其中TaskUmbilicalProtocol为map和reduce对应的Container想appmaster注册的协议。
			sendLaunchedEvents
				发送JobCounterUpdateEvent事件
				发送参数为TaskAttemptStartedEvent的JobHistoryEvent事件
			发送SpeculatorEvent事件
			发送T_ATTEMPT_LAUNCHED类型的TaskTAttemptEvent事件
			

	处理T_ATTEMPT_LAUNCHED类型的TaskTAttemptEvent事件
	TaskStateInternal.SCHEDULED==(TaskEventType.T_ATTEMPT_LAUNCHED,LaunchTransition)==>TaskStateInternal.RUNNING         
		LaunchTransition.transition
			更新metrics等工作
			
/*-----------------以下为远程节点执行的map和reduce任务的情况-----------------*/			
关于具体如何通过startContainers启动一个map的具体参见"yarn应用程序的启动"，这里从启动map的脚本分析一下。
map启动脚本如下:
exec /bin/bash -c "$JAVA_HOME/bin/java -Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN  -Xmx1536M -Djava.io.tmpdir=$PWD/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/data0/yarn/logs/application_1478505603866_0002/container_1478505603866_0002_01_000002 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog org.apache.hadoop.mapred.YarnChild 172.16.177.127 40206 attempt_1478505603866_0002_m_000000_0 2 1>/data0/yarn/logs/application_1478505603866_0002/container_1478505603866_0002_01_000002/stdout 2>/data0/yarn/logs/application_1478505603866_0002/container_1478505603866_0002_01_000002/stderr "
相当于执行如下命令:
java org.apache.hadoop.mapred.YarnChild 172.16.177.127 40206 attempt_1478505603866_0002_m_000000_0 2
reduce启动脚本如下:
exec /bin/bash -c "$JAVA_HOME/bin/java -Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN  -Xmx3072M -Djava.io.tmpdir=$PWD/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/data0/yarn/logs/application_1478505603866_0003/container_1478505603866_0003_01_000004 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog -Dyarn.app.mapreduce.shuffle.logger=INFO,shuffleCLA -Dyarn.app.mapreduce.shuffle.logfile=syslog.shuffle -Dyarn.app.mapreduce.shuffle.log.filesize=0 -Dyarn.app.mapreduce.shuffle.log.backups=0 org.apache.hadoop.mapred.YarnChild 172.16.177.127 46698 attempt_1478505603866_0003_r_000000_0 4 1>/data0/yarn/logs/application_1478505603866_0003/container_1478505603866_0003_01_000004/stdout 2>/data0/yarn/logs/application_1478505603866_0003/container_1478505603866_0003_01_000004/stderr "
相当于执行如下命令:
java org.apache.hadoop.mapred.YarnChild 172.16.177.127 46698 attempt_1478505603866_0003_r_000000_0 4 
			
map任务的分析
YarnChild.main
	一次获取传入参数，依次为host,port,attemptid,appid
	构造jvmid，会根据attempt构造，另外值得注意的是这里根据attemptid的名字来判断该taks是map还是reduce
	... metric,安全相关
	构造TaskUmbilicalProtocol对象umbilical，为了与appmaster通信
	umbilical.getTask							//这里使用rpc远程调用TaskAttemptListenerImpl.getTask
		TaskAttemptListenerImpl.getTask会根据传入的jvmid来返回jvmtask,并跟新相应的列表
	根据上一步返回的jvmtask获取实际的task
	更新YarnChild的taskid
	configureTask
	taskFinal.run								//这里必然是MapTask.run
		MapTask.run
			配置进度条的信息
			初始化reporter
			initialize							//初始化配置信息
			这里好像没有setup阶段，暂时不考虑cleanup阶段
			runNewMapper
				构造taskContext对象
				
				setMapperClass
				...										//一些map具体的执行操作。暂时不分析。
			done
				...
				假如需要提交最终结果，会执行umbilical.commitPending，远程调用TaskAttemptListenerImpl.commitPending, 即发送TaskAttemptEvent(attemptID,TaskAttemptEventType.TA_COMMIT_PENDING)事件
				...
				sendDone
					umbilical.done		远程调用TaskAttemptListenerImpl.done, 即发送TaskAttemptEvent(attemptID, TaskAttemptEventType.TA_DONE))
reduce任务的分析
YarnChild.main
	//具体流程同map任务
	ReduceTask.run
		配置reduce三个阶段的进度，分别为copy，sort，reduce
		初始化reporter
		...												//一些reduce具体的执行操作。暂时不分析。

/*-----------------以上为远程节点执行的map任务的情况-----------------*/			
		
	思考的问题是，这里TaskTAttempt处理RUNNING状态，具体的map的任务必须知道真正执行在remote节点的map任务的执行进度，带着这个问题分析。
		实际上对于Map,会使用父类方法Task.statusUpdate通过TaskUmbilicalProtocol协议给appmaster发送更新进度信息，具体略。
		
这里接下来分析TaskAttempt的状态机，按照需要提交结果的流程分析，即存在COMMIT_PENDING状态的流程。
			
	
TaskAttemptStateInternal.RUNNING==(TaskAttemptEventType.TA_COMMIT_PENDING,CommitPendingTransition)==>TaskAttemptStateInternal.COMMIT_PENDING,
	CommitPendingTransition.transition
		发送类型为TaskEventType.T_ATTEMPT_COMMIT_PENDING的TaskTAttemptEvent事件。	这时候会从TaskAttempt的状态机处理跳到Task的状态机处理中	
		
处理类型为TaskEventType.T_ATTEMPT_COMMIT_PENDING的TaskTAttemptEvent事件
TaskStateInternal.RUNNING==(TaskEventType.T_ATTEMPT_COMMIT_PENDING,AttemptCommitPendingTransition)==>TaskStateInternal.RUNNING
	AttemptCommitPendingTransition.transition
		假如task.commitAttempt为null就设置它。否则表示已经提交过，就发送TaskAttemptKillEvent。	//因此，可以知道该步骤实际是为了告诉task自己这个taskattemp已经完成。

根据MapTask.run的done调用，可以知道还有类型为TaskAttemptEventType.TA_DONE的TaskAttemptEvent信号需要处理
	TaskAttemptStateInternal.COMMIT_PENDING==(TaskAttemptEventType.TA_DONE, CLEANUP_CONTAINER_TRANSITION)==>TaskAttemptStateInternal.SUCCESS_CONTAINER_CLEANUP,
  	CLEANUP_CONTAINER_TRANSITION，即CleanupContainerTransition
  	CleanupContainerTransition.transition
			taskAttemptListener.unregister							//taskattempt马上结束，不要需要与am通信，所以unregister
			如果发来的是TaskAttemptKillEvent，则增加诊断信息
			更新进度表
			updateProgressSplits					//更新进程信息，counter
			发送类型为ContainerLauncher.EventType.CONTAINER_REMOTE_CLEANUP的ContainerLauncherEvent事件

处理类型为ContainerLauncher.EventType.CONTAINER_REMOTE_CLEANUP的ContainerLauncherEvent事件
	会由ContainerLauncherImpl.EventProcessor.run进入CONTAINER_REMOTE_CLEANUP分支处理
	c.kill
		首先会考虑是否launch或是否完成的情况进行处理。
		如果没有launch的时候，更新状态。
		如果没有完成，会rpc远程执行stopContainers关闭container
		最后发送类型为TaskAttemptEventType.TA_CONTAINER_CLEANED的TaskAttemptEvent事件。这里对所有的情况均发送该事件。
		
处理类型为TaskAttemptEventType.TA_CONTAINER_CLEANED的TaskAttemptEvent事件
	TaskAttemptStateInternal.SUCCESS_CONTAINER_CLEANUP==(TaskAttemptEventType.TA_CONTAINER_CLEANED,SucceededTransition)==>TaskAttemptStateInternal.SUCCEEDED
	SucceededTransition.transition
		设置taskAttempt的结束事件
		处理createJobCounterUpdateEventTASucceeded事件
		logAttemptFinishedEvent			//处理日志
		发送类型为TaskEventType.T_ATTEMPT_SUCCEEDED的TaskTAttemptEvent事件
		发送SpeculatorEvent
	
处理类型为TaskEventType.T_ATTEMPT_SUCCEEDED的TaskTAttemptEvent事件
	这个时候TaskStateInternal处于RUNNING状态
	TaskStateInternal.RUNNING==(TaskEventType.T_ATTEMPT_SUCCEEDED,AttemptSucceededTransition)==>TaskStateInternal.SUCCEEDED
		AttemptSucceededTransition.transition
			handleTaskAttemptCompletion						//发送JobTaskAttemptCompletedEvent        
			将taskattemptid加入到task.finishedAttempts
			将taskattemptid从task.inProgressAttempts移除
			更新successfulAttempt为该taskattemptid						//稍后马上使用，为了记录该task下成功的那个attempid. 个人认为用局部变量更好。
			sendTaskSucceededEvents
				发送类型为JobEventType.JOB_TASK_COMPLETED的JobTaskEvent事件，传入一个参数为TaskState.SUCCEEDED
				发送JobHistoryEvent事件
			会遍历task下的所有attempt, 如果不是成功的那个successfulAttempt并没有完成，会发送TaskAttemptKillEvent
				
处理类型为JobEventType.JOB_TASK_COMPLETED的JobTaskEvent事件
	当前JobStateInternal状态为RUNNING
	JobStateInternal.RUNNING==(JobEventType.JOB_TASK_COMPLETED,TaskCompletedTransition)==>JobStateInternal.RUNNING,JobStateInternal.COMMITTING, JobStateInternal.FAIL_WAIT,JobStateInternal.FAIL_ABORT
  	TaskCompletedTransition.transition
  		获取对应的Task
  		taskSucceeded
  			更新succeededMapTaskCount或succeededMapTaskCount
  			更新metric
			checkJobAfterTaskCompletion
				假如map或reduce达到一定的比例(默认是0，即不能失败)失败的话
				设置结束时间
				对所有未结束的任务发送类型为TaskEventType.T_KILL的TaskEvent事件
				假如之前所有的任务完成的话，发送类型为FAILED的CommitterJobAbortEvent事件。返回JobStateInternal.FAIL_ABORT
				然后是一步等待任务被杀的设置
				checkReadyForCommit
					假如所有任务完成，并且当前状态是RUNNING。发送CommitterJobCommitEvent并返回JobStateInternal.COMMITTING状态。

这里按照一直正确的流程继续分析。因此分析处理CommitterJobCommitEvent事件
	交给CommitterEventHandler.run的JOB_COMMIT分支处理
	handleJobCommit
		touchz						//创建startCommitFile文件，作为开始标志。
		jobCommitStarted	//设置当前线程为jobCommitThread
		waitForValidCommitWindow				//保证上次心跳与当前时间不超过10s(默认值)的间隔
		committer.commitJob							//根据具体的类做一些操作和清理操作
		touchz						//创建endCommitSuccessFile文件，作为成功标志
		发送JobCommitCompletedEvent
		jobCommitEnded

处理JobCommitCompletedEvent事件

JobEventType.JOB_COMMIT_COMPLETED
	JobStateInternal.COMMITTING==((JobEventType.JOB_COMMIT_COMPLETED,CommitSucceededTransition))==>JobStateInternal.SUCCEEDED
    CommitSucceededTransition.transition
    	发送JobFinishedEvent事件
    	job.finished
    		更新metrics    
				
至此，一个MR程序的正常运行流程分析完成			
4 组件和一些重要的函数的说明
4.1 JobHistory说明
使用类: org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer

4.2 ContainerAllocatorRouter组件说明
		ContainerAllocatorRouter.serviceStart
			这里暂时不考虑uber模式
			构造RMContainerAllocator对象containerAllocator
			RMContainerAllocator.serviceInit
				根据配置项mapreduce.job.reduce.slowstart.completedmaps获取reduceSlowStart，默认0.05
				根据配置项yarn.app.mapreduce.am.job.reduce.rampup.limit获取maxReduceRampupLimit，默认0.5
				根据配置项yarn.app.mapreduce.am.job.reduce.preemption.limit获取maxReducePreemptionLimit，默认0.5
				根据配置项mapreduce.job.reducer.preempt.delay.sec获取allocationDelayThresholdMs，默认0，会乘以1000转化为ms
				根据配置项mapreduce.job.running.map.limit获取maxRunningMaps，默认0
				根据配置项mapreduce.job.running.reduce.limit获取maxRunningReduces，默认0
				RackResolver.init
					获取net.topology.node.switch.mapping.impl的值作为时间类dnsToSwitchMappingClass，默认值为ScriptBasedMapping
					构造dnsToSwitchMappingClass实例
					最后构造得到dnsToSwitchMapping
				根据配置项yarn.app.mapreduce.am.scheduler.connection.wait.interval-ms获取retryInterval，默认360s
				设置当前时间为retrystartTime
			RMContainerAllocator.serviceStart
				循环从eventQueue获取事件,为阻塞调用,当有事件到来,使用handleEvent处理。								//具体的事件处理稍后分析

4.3 ApplicationMasterService.allocate的说明
ApplicationMasterService.allocate
	获取appattemptid和appid
	为amLivelinessMonitor发ping响应存活信息
	判断app是否注册
	判断心跳是否是新的
	规范化request的progress
	发送RMAppAttemptStatusupdateEvent
	获取请求列表ask和释放列表release以及blacklistRequest					//暂不分析blacklistRequest
	...	nodelabel， 规范化等
	rScheduler.allocate					//回来领取rm调度器分配的资源，调用FairScheduler.allocate
		得到对应的应用的结构体
		updateResourceRequests
			
		...				//其他信息
	构造allocateResponse
	配置nodereport,AllocatedContainers,CompletedContainersStatuses,AvailableResources,NumClusterNodes等信息
	
4.4 RMCommunicator.scheduler
RMCommunicator.scheduler完成了ApplicationMasterProtocol协议，是AM与RM通信的主要协议。
实际实现为ApplicationMasterProtocolPBClientImpl类


附录A 配置的说明
mapreduce.client.completion.pollinterval				表示客户端循环判断job是否完成的时间间隔
mapreduce.job.reduces														设置reduce的数目
mapreduce.job.reduce.slowstart.completedmaps

默认的OutputFormat为TextOutputFormat


FileOutputCommitter






附录A:
命令:
(1) hdfs dfs -put a.txt /tmp/in						注:这里txt为653M左右
(2) hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar wordcount /tmp/in /tmp/out

集群threshold参数默认为1，maxassign为1
部分命令的打印信息
[jd_ad@BJYF-Druid-15221 ~]$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar wordcount /tmp/in /tmp/out
Not a valid JAR: /home/jd_ad/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar
[jd_ad@BJYF-Druid-15221 ~]$ cd /software/servers/hadoop-2.7.1/
[jd_ad@BJYF-Druid-15221 hadoop-2.7.1]$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar wordcount /tmp/in /tmp/out
[2016-10-27T17:31:57.847+08:00] [INFO] yarn.client.ConfiguredRMFailoverProxyProvider.performFailover(ConfiguredRMFailoverProxyProvider.java 100) [main] : Failing over to rm2
[2016-10-27T17:31:58.504+08:00] [INFO] lib.input.FileInputFormat.listStatus(FileInputFormat.java 283) [main] : Total input paths to process : 1
[2016-10-27T17:31:58.518+08:00] [INFO] compression.lzo.GPLNativeCodeLoader.<clinit>(GPLNativeCodeLoader.java 52) [main] : Loaded native gpl library from the embedded binaries
[2016-10-27T17:31:58.520+08:00] [INFO] compression.lzo.LzoCodec.<clinit>(LzoCodec.java 76) [main] : Successfully loaded & initialized native-lzo library [hadoop-lzo rev 40b332285be4d4a9ec95a02694b20211810cf4ec]
[2016-10-27T17:31:58.839+08:00] [INFO] hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java 233) [main] : number of splits:3
[2016-10-27T17:31:59.081+08:00] [INFO] hadoop.mapreduce.JobSubmitter.printTokens(JobSubmitter.java 322) [main] : Submitting tokens for job: job_1477447398282_0058
[2016-10-27T17:31:59.441+08:00] [INFO] api.impl.YarnClientImpl.submitApplication(YarnClientImpl.java 273) [main] : Submitted application application_1477447398282_0058
[2016-10-27T17:31:59.493+08:00] [INFO] hadoop.mapreduce.Job.submit(Job.java 1294) [main] : The url to track the job: http://BJYF-Druid-15216.hadoop.jd.local:50320/proxy/application_1477447398282_0058/
[2016-10-27T17:31:59.494+08:00] [INFO] hadoop.mapreduce.Job.monitorAndPrintJob(Job.java 1339) [main] : Running job: job_1477447398282_0058
[2016-10-27T17:32:16.739+08:00] [INFO] hadoop.mapreduce.Job.monitorAndPrintJob(Job.java 1360) [main] : Job job_1477447398282_0058 running in uber mode : false
[2016-10-27T17:32:16.742+08:00] [INFO] hadoop.mapreduce.Job.monitorAndPrintJob(Job.java 1367) [main] :  map 0% reduce 0%

map的信息
attempt_1477447398282_0058_m_000000_0	SUCCEEDED	map	/default-rack/BJYF-Druid-15219.hadoop.jd.local:8042	logs	Thu Oct 27 17:32:17 +0800 2016	Thu Oct 27 17:32:52 +0800 2016	34sec	
attempt_1477447398282_0058_m_000001_0	SUCCEEDED	map	/default-rack/BJYF-Druid-18531.hadoop.jd.local:8042	logs	Thu Oct 27 17:32:17 +0800 2016	Thu Oct 27 17:32:52 +0800 2016	34sec	
attempt_1477447398282_0058_m_000002_0	SUCCEEDED	map	/default-rack/BJYF-Druid-184126.hadoop.jd.local:8042	logs	Thu Oct 27 17:32:17 +0800 2016	Thu Oct 27 17:32:43 +0800 2016	25sec

reduce的信息
attempt_1477447398282_0058_r_000000_0	SUCCEEDED	reduce > reduce	/default-rack/BJYF-Druid-184122.hadoop.jd.local:8042	logs	Thu Oct 27 17:32:54 +0800 2016	Thu Oct 27 17:33:07 +0800 2016	Thu Oct 27 17:33:08 +0800 2016	Thu Oct 27 17:33:11 +0800 2016	12sec	1sec	3sec	16sec	

fsck信息
hdfs fsck /tmp/in/a.txt -files -locations -blocks   
Connecting to namenode via http://BJYF-Druid-15216.hadoop.jd.local:50070/fsck?ugi=jd_ad&files=1&locations=1&blocks=1&path=%2Ftmp%2Fin%2Fa.txt
FSCK started by jd_ad (auth:SIMPLE) from /172.19.152.21 for path /tmp/in/a.txt at Thu Oct 27 17:41:25 CST 2016
/tmp/in/a.txt 684509280 bytes, 6 block(s):  OK
0. BP-1089191954-172.19.152.15-1474462508760:blk_1073879805_141254 len=134217728 repl=3 [DatanodeInfoWithStorage[172.19.152.19:50010,DS-e12c3e4e-ca10-4633-8c22-f2398fe198f7,DISK], DatanodeInfoWithStorage[172.16.184.120:50010,DS-cb0a8ae4-255c-4bdf-8cf8-3c7066d6dc61,DISK], DatanodeInfoWithStorage[172.16.184.126:50010,DS-5045618b-3107-4ebe-856d-ae1b25710258,DISK]]
1. BP-1089191954-172.19.152.15-1474462508760:blk_1073879806_141255 len=134217728 repl=3 [DatanodeInfoWithStorage[172.16.185.33:50010,DS-b09bb078-d82a-4c3d-b37a-e09e25f9fbac,DISK], DatanodeInfoWithStorage[172.19.152.20:50010,DS-70c45b7a-0e6c-4390-9368-2423f558eb6a,DISK], DatanodeInfoWithStorage[172.16.185.23:50010,DS-48ee9c8d-8d11-4445-8623-8f3482b34897,DISK]]
2. BP-1089191954-172.19.152.15-1474462508760:blk_1073879807_141256 len=134217728 repl=3 [DatanodeInfoWithStorage[172.19.152.19:50010,DS-2f6ea0c2-2ff0-4c27-a249-8da8fda15678,DISK], DatanodeInfoWithStorage[172.16.185.31:50010,DS-0b387221-c615-4bd0-92ab-3ab82e2e58cd,DISK], DatanodeInfoWithStorage[172.16.184.122:50010,DS-c6024932-c060-445e-86cf-8fbdd772a08b,DISK]]
3. BP-1089191954-172.19.152.15-1474462508760:blk_1073879808_141257 len=134217728 repl=3 [DatanodeInfoWithStorage[172.16.184.130:50010,DS-33115c48-e357-409a-bb70-f99cec24d1f4,DISK], DatanodeInfoWithStorage[172.16.185.21:50010,DS-57e8f88f-a98e-4257-badf-791d05d67426,DISK], DatanodeInfoWithStorage[172.16.184.110:50010,DS-b524823d-056a-4fbb-aca5-8afd09efd4c5,DISK]]
4. BP-1089191954-172.19.152.15-1474462508760:blk_1073879809_141258 len=134217728 repl=3 [DatanodeInfoWithStorage[172.16.184.120:50010,DS-dfa93948-94bd-43b0-998b-53beaf0ce862,DISK], DatanodeInfoWithStorage[172.16.184.110:50010,DS-af3a7f78-e772-455a-8106-a5c57778646e,DISK], DatanodeInfoWithStorage[172.16.184.124:50010,DS-1db86050-d376-4596-b318-7521b2b7fa3d,DISK]]
5. BP-1089191954-172.19.152.15-1474462508760:blk_1073879810_141259 len=13420640 repl=3 [DatanodeInfoWithStorage[172.16.185.33:50010,DS-7e31ce5f-66e1-49a0-bdd0-a854a459490e,DISK], DatanodeInfoWithStorage[172.16.185.21:50010,DS-5d3d13a8-65ee-4377-9f32-b76ff90ec93a,DISK], DatanodeInfoWithStorage[172.16.185.29:50010,DS-ad4aa826-1a68-4297-9c4d-cd4bff1284e0,DISK]]
Status: HEALTHY
 Total size:    684509280 B
 Total dirs:    0
 Total files:   1
 Total symlinks:                0
 Total blocks (validated):      6 (avg. block size 114084880 B)
 Minimally replicated blocks:   6 (100.0 %)
 Over-replicated blocks:        0 (0.0 %)
 Under-replicated blocks:       0 (0.0 %)
 Mis-replicated blocks:         0 (0.0 %)
 Default replication factor:    3
 Average block replication:     3.0
 Corrupt blocks:                0
 Missing replicas:              0 (0.0 %)
 Number of data-nodes:          28
 Number of racks:               1
FSCK ended at Thu Oct 27 17:41:25 CST 2016 in 1 milliseconds
The filesystem under path '/tmp/in/a.txt' is HEALTHY


hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar wordcount -Dyarn.app.mapreduce.am.resource.cpu-vcores=10 /tmp/in /tmp/out






附录B: terasort本地化请求信息记录
生成初始材料:
hadoop jar /software/servers/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar terasort /tmp/out2 /tmp/sort2

生成文件的hdfs副本信息:
hdfs fsck /tmp/out2 -locations -blocks -files   
/tmp/out2/part-m-00000 500000 bytes, 1 block(s):  OK
0. BP-1388297707-172.16.177.125-1473941304741:blk_1073869409_128649 len=500000 repl=3 [DatanodeInfoWithStorage[172.16.176.49:50010,DS-dffccd77-1b1b-4648-b0a5-a8f0e83cb523,DISK], DatanodeInfoWithStorage[172.16.176.47:50010,DS-6aaf2da2-6450-4328-8fcd-3795af378a4f,DISK], DatanodeInfoWithStorage[172.16.177.127:50010,DS-0c0fec4c-8e5a-496d-b012-55c5f0e36905,DISK]]
/tmp/out2/part-m-00001 500000 bytes, 1 block(s):  OK
0. BP-1388297707-172.16.177.125-1473941304741:blk_1073869410_128650 len=500000 repl=3 [DatanodeInfoWithStorage[172.16.176.49:50010,DS-ae4f8cc1-fa03-4a34-a9de-49ca7de99df0,DISK], DatanodeInfoWithStorage[172.16.177.127:50010,DS-183a5bba-0dd7-4485-a138-4c3c42a5027d,DISK], DatanodeInfoWithStorage[172.16.176.47:50010,DS-3cbc3787-64e9-424c-bf25-abc31c666385,DISK]]
可以看出两个文件都是三个副本(该测试集群只有3个DN和NM节点)

命令:
hadoop jar /software/servers/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar terasort /tmp/out2 /tmp/sort2






问题:
一个map对应的split有3个副本，会对应几个request




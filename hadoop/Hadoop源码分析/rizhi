20日 周二:
	事件: 上线 198台机器
	现象:
		1.callqueue有些许震荡
		2.队列有资源, 但依然pending
	结果:
		未发现,无处理
     -----------------------------------------------------
23日 周五:
	事件: 迁移CMO及时数据
	现象:
		1.出现队列可分配资源有负数情况(23日出现)
		2.队列有资源, 但依然pending
	措施:
		1.打入两个社区的putch
		2.上线方式, 更新standy , 切换重启
	结果:
		集群出现callqueue居高不下, 任务提交后,无法生成jobId
     -----------------------------------------------------
	现象:
		1.集群出现callqueue居高不下, 任务提交后,无法生成jobId
	措施:
		1.重启yarn,包括RM,NM
	结果:
		无效
		集群出现callqueue居高不下, 任务提交后,无法生成jobId
     -----------------------------------------------------
	现象:
		1.集群出现callqueue居高不下, 任务提交后,无法生成jobId
	措施:
		1.修改配置
			yarn.resourcemanager.work-preserving-recovery.enabled true -> false 
		2.重启RM
	结果:
		无效
     -----------------------------------------------------
	现象:
		1.集群出现callqueue居高不下, 任务提交后,无法生成jobId
	措施:
		1.修改配置
			yarn.resourcemanager.work-preserving-recovery.enabled true -> false 
		2.重启RM
	结果:
		无效
     -----------------------------------------------------
    现象:
		1.集群出现callqueue居高不下, 任务提交后,无法生成jobId
	措施:
		1.修改配置 关闭RM HA
			yarn.resourcemanager.work-preserving-recovery.enabled  true -> false 
		2.重启RM
	结果:
		无效
     -----------------------------------------------------
    现象:
		1.集群出现callqueue居高不下, 任务提交后,无法生成jobId
	措施:
		1.修改配置 关闭RM HA
			yarn.resourcemanager.recovery.enabled true -> false 
		2.重启RM
	结果:
		无效
		22点 - 3点 正常, 3点以后出现appid获取慢
     -----------------------------------------------------
24日 周六:

    现象:
		1.集群出现callqueue居高不下, 任务提交后,无法生成jobId
	措施:
		1.下200个节点,重启RM
	结果:
		无效
     -----------------------------------------------------
    现象:
		1.集群出现callqueue居高不下, 任务提交后,无法生成jobId
	措施:
		1.修改配置 
			1.增加handle
			2.增加faircallqueue
		yarn.resourcemanager.scheduler.client.thread-count 200 -> 250
		yarn.resourcemanager.client.thread-count 100 ->150
		ipc.8032.callqueue.impl  无  -> org.apache.hadoop.ipc.FairCallQueue
		ipc.8032.faircallqueue.decay-scheduler.period-ms 无 -> 60000
	结果:
		无效
     -----------------------------------------------------
    现象:
		1.集群出现callqueue居高不下, 任务提交后,无法生成jobId
	措施:
		1.修改配置  
			1.修改log4j参数为error
			log4j.threshold=ERROR
	结果:
		无效, 11:00 - 12:00 正常
		12:00 增加200节点,随即现象复现
     -----------------------------------------------------
    现象:
		1.集群出现callqueue居高不下, 任务提交后,无法生成jobId
	措施:
		1.切换RM服务器, 将65切换到66
	结果:
		16:00 - 次日6:00,正常
		次日6点以后出现appid获取慢
     -----------------------------------------------------
25日 周日
	现象:
		1.集群出现callqueue居高不下, 任务提交后,无法生成jobId
	措施:
		1.上线2个优化patch
	结果:
		无效
     -----------------------------------------------------		
    现象:
		1.集群出现callqueue居高不下, 任务提交后,无法生成jobId
	措施:
		1.上线1个优化patch
	结果:
		无效
     -----------------------------------------------------	
    现象:
		1.集群出现callqueue居高不下, 任务提交后,无法生成jobId
	措施:
		1.调整NM汇报心跳事件
		yarn.resourcemanager.nodemanagers.heartbeat-interval-ms 1000 -> 10000
	结果:
		任务正常通过, 进行压测, 2000节点, 压测到并发job数1600正常
     -----------------------------------------------------	
26日 周一
	现象:
		1.上午9点开始出现断断续续的卡顿
		2.队列资源充足,但无法利用上
		3.Apps Pending数非常多
	措施:
		凌晨 12点: 执行回滚至23日操作
		与 23日差异:
			      23日  26日
		yarn ha    有    无 
		心跳       1s    10s
	结果:
		压测, 通过
	 -----------------------------------------------------	
27日 周二
	现象:
		1.任务并行度有所提升, Pending问题解决, 同时runApps最高达到800以上
		2.任务执行过程较为迟缓,业务方任务较26日[周一]慢2-3倍
	措施:
		9:00 将配置及jar包恢复至26日[周一],重启RM
	结果:
		无效
     -----------------------------------------------------
	现象:
		1.Pending数非常多, 任务执行过程较为迟缓, 两者兼具, 较之9:00以前更迟缓
	措施:
		14:00 将配置和jar包完全回滚23日
	结果:
		无效
     -----------------------------------------------------
	现象:
		1.申请jobId相应慢, callqueue高
	措施:
		1. 18:00 修改NM的cpu和内存配比 28/96 -> 48/96
		2. yarn.resourcemanager.nodemanagers.heartbeat-interval-ms 1000 -> 5000
		3. 修改fairSchedule.xml与jmartad一致
	结果:
		callqueue偶尔高, 杀死runJob,能减轻callqueue压力,可以恢复正常状态
		压测通过
     -----------------------------------------------------
28日 周三
	现象 :
		1.callqueue上下震荡剧烈,任务申请jobId部分有延迟
		2.10:10 上线198台机器, 增加计算资源
		3.11:00 callqueue 居高不下,无法自愈
	措施:
		进行重启rm
	结果:
		临时处理手段, 撑过上午的高峰
     -----------------------------------------------------	
	现象 :
		14:00 callqueue上下震荡剧烈,任务申请jobId部分有延迟
		问题依然存在
	措施:
		完全还原至20日的生产配置和计算节点数
		1.将上线198台机器, 下线
		2.配置还原, 48/96 -> 28/56
		3.重启yarn(rm,nm)
		4.将集中在9:00的app,平均打散在不同时间点启动
		5.修改配置
			yarn.scheduler.fair.max.assign 5 -> -1
			yarn.admin.acl 空格 -> *

	结果:
		发现有部分任务出现远程读现象
		压测通过
     -----------------------------------------------------
29日 周四
	现象 :
		1.4:00 发现job执行缓慢,有一批任务一直进行远程读
	措施:
		修改配置
			yarn.scheduler.fair.max.assign -1 -> 5
		5:55 重启rm
	结果:
		无效[未知],远程读job于6点后渐少
     -----------------------------------------------------	
	现象:
		远程读job于6点后渐少, 但部分DN所在的节点网络打满,任务依然慢
	措施: 
		1.kill 网络高的DN, 迫使读取第二副本数据, 任务快速通过
		2.增加公共数据副本 2 -> 4
	结果:
		集群,平安度过
     -----------------------------------------------------
30日 周五	
	现象 :
		1.少量任务因为DN所在的节点网络打满二运行缓慢(相较28日,量级下降很多)_
	措施: 
		1.kill 网络高的DN, 迫使读取第二副本数据, 任务快速通过
	结果:
		集群,正常度过
     -----------------------------------------------------
1日 周六	
	现象 :
		1.部分DN所在的节点网络打满, 过数分钟后自愈, 网络恢复正常
	措施: 
		1.无,跟踪
	结果:
		集群1号,正常
     -----------------------------------------------------
    措施: 
		1.上线60台RM


